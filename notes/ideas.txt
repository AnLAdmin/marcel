
Functions and ops

- There is already a ps command and a processes() function.

- There is a cd command. There is no cd function, which would be
  useful, e.g. to simulate du -hs:

  ls -d | cd(...) | ls -fr(...) | map (f: (1, f.size)) | red + +

For each directory in current directory: cd to it, then recursively
find files, counting and summing sizes.

Needs ls and cd working as functions.




ls -d | map (dir: ls(-fr, dir)) | expand | map (f: (1, f.size)) | red + +


ls -d | (dir: (dir, ls('-fr', dir)) | expand 1 | (f: (dir, 1, f.size)) | red . + +

- An explicit "map" is noisy and can be inferred.

- But inside the function, we're supposed to have pure python. How are
  flags specified? (ls -fr)

- Environment variables: 

    ls -d | (dir: ls -fr $dir) | ...

  yechh. It means we have two kinds of variables, two kinds of
  function. And the function syntax is now no longer always python.

  But that's the case anyway, if we use "ls -fr | ..." on the command
  line and "ls('-fr') | ..." in Python.

  Hmm.

What about Python syntax everywhere?

  (ls('-d')) | (dir: (dir, ls('-fr', dir))) | expand 1 | (dir, f: (dir, 1, f.size)) | red . + +


Maybe shell syntax is defined to be syntactic sugar for function invocation. So 

      ls -fr d/*

is equivalent to

      (ls('-fr d/*'))

And either is acceptable as input? The function then has to yield a
stream, not a sequence.

Could have a "stream" operator, e.g. stream (ls()), but that's
basically "ls() | expand". And stream is less general.

----------------------------------------------------------------------

send/receive vs. yield

Could write operators as python generators, providing output via
yield. For operators other than the first, an input source is
needed. E.g., here is map.receive:

    def receive(self, x):
        self.send(self.f(*x))

Instead, it would be

    def run(self):
        try:
            input = self.input.run()
            while True:
                yield self.f(next(input))
        except StopIteration:
            ...

So instead of calling self.send (implying something downstream), you
know about the thing upstream (self.input). And yield is native
python, not an osh API.

So for the first item in a pipeline, it would just be "yield", and
there is no dependence on other operators. This allows ps(), ls(),
etc. to be used as functions or operators.
        
----------------------------------------------------------------------

Realization!

processes() is a function returning a list of Process. ps() is a
command generating a stream of Process. They can't be unified without
making processes() generate a stream, perhaps using yield. But in
other situations, we really want ordinary functions, e.g. for use in
map and select.

So:

ls:

- The kinds of functions used in map and select are ordinary python
  functions. Leave those alone, they are fine as is.

- If we want to unify ps()/ps, or ls()/ls, then we need python
  generators. 

Here is the implementation of ls()

- It creates an instance of class Ls.

- Ls extends Op.

- Ls has fields corresponding to the flags of the ls operator, and a
  few others.

- Ls has a __doc__ string, returned by doc().

- Initialization is done by setup() (after flag parsing)

- Ls has a parser, and returns it using arg_parser()

- execute() does the work, eventually calling send()

ps:

Similar to the above, but execute simply invokes processes(). Which
returns a list. It could easily be reformulted to be a generator.

----------------------------------------------------------------------

Unifying API and CLI:

- Instead of ls() -> Ls() -> setup(): parser could hold on to args,
  and then just invoke ls(args). Ls() could have parent class do arg
  parsing. Get rid of setup, doing post-parse work in Ls().

======================================================================

cat is "ls | expand". Add an abstraction mechanism? 

Yes, see pipelines as first-class objects:

cat = [f: ls f | expand]

======================================================================


Environment variables:

- current directory
- current thread (in case of a fork)
- Prompts (ps1, ps2)

----------------------------------------------------------------------

Doing more with pipelines:

Pipelines should be first-class objects:

- A form of abstraction: Assign a pipeline to a variable, then use it
  in commands and to build other pipelines.

- Pipelines can be passed to operators. E.g. there could be a join
  operator, taking two pipelines as inputs. In fact, the remote fork
  is such an operator.

File counts and sizes under current directory:

    file_counts_and_sizes = [ ls -fr | map (f: (1, f.size)) | red + + ]
    file_counts_and_sizes  # executes the pipeline

In a given directory:

    file_counts_and_sizes = [dir: ls -fr dir | map (f: (1, f.size)) | red + + ]

    file_counts_and_sizes '/tmp/foobar'

Composing:

    python_processes = [ ps | select (p: 'python' in p.commandline) ]
    python_processes | map (p: (p.pid, p.size, p.state))

Join, with args literal pipeline args:

    ext_sizes = [dir: ls -fr dir | map (f: (f.extension, f.size)) ]
    join [ ext_sizes '/home/jao' ] (L, R: L.extension == R.extension) [ ext_size '/home/alo' ]

capture op:

        capture PREDICATE destination

Apply predicate. If true, send to destination. Otherwise, send
downstream. Might want an option that sends everything downstream,
including things that are captured.

So this is basically if/then/else, (or if/then if the true items go
downstream also).

The destination could be a file or a pipeline.

Cascading these is a switch:

    if p1 [pipe1] | if p2 [pipe2] | if p3 [pipe3] | ...

----------------------------------------------------------------------

- Allow cd to be mid-pipeline. It establishes an env that applies
  downstream only. I.e., logically, each op establishes a new COPY of
  the env, and can make changes not visible upstream.

----------------------------------------------------------------------

Files & times:

Getting a file's mtime is kind of a pain, e.g. (f:
f.stat().st_mtime). That's a floating point number representing
seconds since the epoch. Might want to improve in two ways:

1) File.mtime attribute.

2) Type of File.mtime is a Time object, that has convenient methods
for rendering, construction, deltas. 

But #2 starts overlapping with Python time, datetime, etc. Don't want
to do that, but we do want something for use on the command
line. E.g., select files not modified in the past year.

----------------------------------------------------------------------

A problem with filename based commands is that things get messy due to
quoting, escaping, spaces in filenames, etc. These problems go away if
they work on Files. So ls finds the files by name, but then we have a
stream of Files, not filenames separated by spaces that may contain
spaces.

----------------------------------------------------------------------

Allow files to be piped into ls?

----------------------------------------------------------------------

Pipelines with arguments:

E.g.

        x = [ls -fr /some/dir]

To use it:

        x

Add an argument:

        x = [xyz: ls -fr xyz]

But how to distinguish the xyz argument from an actual path xyz?

As a function!

        x = [xyz: ls -fr (xyz)]

- [xyz: ...] adds xyz to the local namespace (for the pipeline).

- (xyz) is an ordinary function invocation. Need to generalize ls arg
  processing to allow for it.

- Use:

        x /some/dir

----------------------------------------------------------------------

timer could take an optional pipeline arg. If present, run it every
second, making the timestamp available. PIPELINE ARGS!

E.g., Run timer, run ps every tick and label ps output. This works:

    timer 1 | map (t: (t, processes())) | expand 1

Which gets into the discussion above about functions and ops.

----------------------------------------------------------------------

Args to pipeline vars:

E.g., a pipeline that finds files with a given extension.

To get python files:

    py = [select (f: f.suffix == '.py')]
    ls -fr | py

To specify the extension of interest:

    ext = [select (f: f.suffix == e)]
    ls -fr | ext -e '.py'

I.e., in the ext pipeline 'e' is a variable. It is bound to '.py' by
specifying the arg as a flag. It is currently OK for e not to be
defined when the pipeline is defined.

WHAT WOULD THE TYPE OF e BE? Assume string; Use an expr to construct a
value of a specific type, e.g. "foo -x (2)"

----------------------------------------------------------------------

Data pipeline:

Imagine an if operator:

... | if PREDICATE-1 VAR-1 | if PREDICATE-2 VAR-2 | ...

- var-n captures the incoming stream for which predicate-n is true.
- Data is stored and can be reused later, e.g.

  var-1 | ...

Could also save data, e.g. "out -s|--save"

  gen 10 | out -s n10
  n10 | ...

- if: send everything downstream, regardless of predicate value.

- ifelse: send x downstream if predicate(x) is false.

- if without var: send x downstream if predicate(x) is true, i.e. select op.


Replace var-n with a pipeline, which could save results. So

    gen 10 | ifelse (x: x%2 == 0) [out -s evens] | out -s odds
