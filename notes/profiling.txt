5/21/20

    N = 1000000
    start = time.time()
    run(gen(N) | map(lambda x: x+1) | map(lambda x: x+1) | map(lambda x: x+1) | select(lambda x: False))
    stop = time.time()
    usec = (stop - start) * 1000000 / N
    print(f'{usec} usec per unit')

Takes about 5.5 usec per unit.

Profiling shows 31% of time in normalize_op_input. 24% in
is_sequence_except_string.

    def normalize_op_input(x):
        return (None if x is None else
                tuple(x) if is_sequence_except_string(x) else
                (x,))

This cuts time to 4.4 usec per unit:

def normalize_op_input(x):
    t = type(x)
    return (None if x is None else
            x if t is tuple else
            tuple(x) if t is list else
            (x,))

Testing for tuple first is about the same, maybe very slightly slower:

def normalize_op_input(x):
    t = type(x)
    return (x if t is tuple else
            tuple(x) if t is list else
            None if x is None else
            (x,))

normalize_op_input is now 8.6%.

----------------------------------------------------------------------

receive_input OWN time is 26.6%:

    def receive_input(self, x):
        assert not isinstance(x, Error)
        try:
            self.receive(marcel.util.normalize_op_input(x))
        except marcel.exception.KillAndResumeException as e:
            self.receive_error(e.error)

Without the assert, time drops to about 4.1 usec, 22.6%.

Inlining normalize_op_input is a big win:

- 3.5 usec
- 21%

----------------------------------------------------------------------

Disable assertion in send:

- 3.2 -> 2.7 usec
- 21% -> 17%

----------------------------------------------------------------------

Seeing big own times in send and receive_input. Probably function call
overhead, as with inlining normalize_op_input.

Should be able to go a lot faster buffering input. Sent around batches
of input.

This is a big change, as it modifies receive for EVERY op, and
requires each to iterate over input.
